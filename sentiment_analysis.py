# -*- coding: utf-8 -*-
"""Sentiment Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cMW_3-jipTqy9-dLdbr483nAmtHeYqcG
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from wordcloud import WordCloud,STOPWORDS

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.naive_bayes import GaussianNB, MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, plot_confusion_matrix, plot_roc_curve, plot_precision_recall_curve
from xgboost.sklearn import XGBClassifier

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import keras
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from keras.models import Sequential
from keras.layers import Dense, Embedding, LSTM, GRU, SimpleRNN
from keras.layers.embeddings import Embedding

from google.colab import drive
drive.mount('/content/drive')

"""## Data Extraction and Cleaning"""

data = pd.read_csv('/content/drive/MyDrive/IMDB Dataset.csv')
data.head()

data.shape

data.info()

data['sentiment'].value_counts()

null_values = data.isnull().sum()
null_values

"""No null values present in dataset"""

num_duplicates = data.duplicated().sum() #identify duplicates
print('There are {} duplicate reviews present in the dataset'.format(num_duplicates))

#view duplicate reviews
review = data['review']
duplicated_review = data[review.isin(review[review.duplicated()])].sort_values("review")
duplicated_review.head()

#drop duplicate reviews
data.drop_duplicates(inplace = True)

print('The dataset contains {} rows and {} columns after removing duplicates'.format(data.shape[0],data.shape[1]))

data['review'][100]

"""Text Cleaning

1. Removing the special characters
2. Lowering the text.
3. Lemmatization
4. Removing the stop words
"""

import math
import nltk
import matplotlib.pyplot as plt
import seaborn as sns
nltk.download('stopwords')
nltk.download('wordnet')

from nltk.corpus import stopwords
new_stopword = ['br']
stwrd = nltk.corpus.stopwords.words('english')
stwrd.extend(new_stopword)

import re
from nltk.stem import WordNetLemmatizer
wl = WordNetLemmatizer()

def text_preprocessing(text):
    text = re.sub('[^a-zA-Z]',' ',text)
    text = text.lower()
    text = text.split()
    text = [wl.lemmatize(word) for word in text if not word in stwrd]
    text = ' '.join(text)
    return text

data['review'] = data['review'].apply(text_preprocessing)

data['review'][100]

data.sentiment = [ 1 if each == "positive" else 0 for each in data.sentiment]

"""## Exploratory Data Analysis"""

# Count Plot
sns.countplot(data['sentiment'])

positive = data[data['sentiment'] == 1]
negative = data[data['sentiment'] == 0]

per_positive = (len(positive)/len(data['sentiment']))*100
per_negative = (len(negative)/len(data['sentiment']))*100
print(f'Positive and negative reviews in percentage are {per_positive} % and {per_negative} %')

"""This is balanced data.

Creating Word Cloud for Positive and Negative reviews, So we can see the importance of words.
"""

# For Positive reviews
from wordcloud import WordCloud
positive_text = ' '.join(str(cat.split()) for cat in positive['review'])
positive_cloud = WordCloud(collocations = False, background_color = 'white',max_words=50).generate(positive_text)
plt.figure(figsize=(10,8))
plt.imshow(positive_cloud,interpolation='bilinear')
plt.axis('off')
plt.show()

# For negative reviews
negative_text = ' '.join(str(cat.split()) for cat in negative['review'])
negative_cloud = WordCloud(collocations = False, background_color = 'white',max_words=50).generate(negative_text)
plt.figure(figsize=(10,8))
plt.imshow(negative_cloud,interpolation='bilinear')
plt.axis('off')
plt.show()

positive_data = data[data.sentiment == 1]['review']
positive_data_string = ' '.join(positive_data)
negative_data = data[data.sentiment == 0]['review']
negative_data_string = ' '.join(negative_data)
fig,(ax1,ax2)=plt.subplots(1,2,figsize=(8,6))
text_len=positive_data.str.len()
ax1.hist(text_len,color='green')
ax1.set_title('Positive Reviews')
ax1.set_xlabel('Number of Characters')
ax1.set_ylabel('Count')
ax1.set_xlim(0,6000)
ax1.set_ylim(0,18000)
text_len=negative_data.str.len()
ax2.hist(text_len,color='red')
ax2.set_title('Negative Reviews')
ax2.set_xlabel('Number of Characters')
ax2.set_ylabel('Count')
ax2.set_xlim(0,6000)
ax2.set_ylim(0,18000)
fig.suptitle('Number of characters in texts')
plt.show()

fig,(ax1,ax2)=plt.subplots(1,2,figsize=(8,6))

text_len=positive_data.str.split().map(lambda x: len(x))
ax1.hist(text_len,color='green')
ax1.set_title('Positive Reviews')
ax1.set_xlabel('Number of Words')
ax1.set_ylabel('Count')
ax1.set_xlim(0,600)
ax1.set_ylim(0,18000)
text_len=negative_data.str.split().map(lambda x: len(x))
ax2.hist(text_len,color='red')
ax2.set_title('Negative Reviews')
ax2.set_xlabel('Number of Words')
ax2.set_ylabel('Count')
ax2.set_xlim(0,600)
ax2.set_ylim(0,18000)

fig.suptitle('Number of words in texts')
plt.show()

fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,8))
word = positive_data.str.split().apply(lambda x : len(x) )
sns.distplot(word, ax=ax1,color='green')
ax1.set_title('Positive Reviews')
ax1.set_xlabel('Number of words per review')
ax1.set_xlim(0,800)
ax1.set_ylim(0,0.012)

word = negative_data.str.split().apply(lambda x :len(x) )
sns.distplot(word,ax=ax2,color='red')
ax2.set_title('Negative Reviews')
ax2.set_xlabel('Number of words per review')
ax2.set_xlim(0,800)
ax2.set_ylim(0,0.012)

fig.suptitle('Distribution of number of words per reviews')
plt.show()

fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,8))
word = positive_data.str.split().apply(lambda x : [len(i) for i in x] )
sns.distplot(word.map(lambda x: np.mean(x)), ax=ax1,color='green')
ax1.set_title('Positive Reviews')
ax1.set_xlabel('Average word length per review')
ax1.set_xlim(0,9)
ax1.set_ylim(0,1.2)
word = negative_data.str.split().apply(lambda x : [len(i) for i in x] )
sns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='red')
ax2.set_title('Negative Reviews')
ax2.set_xlabel('Average word length per review')
ax2.set_xlim(0,9)
ax2.set_ylim(0,1.2)
fig.suptitle('Distribution of average word length in each review')
plt.show()

def get_corpus(text):
    words = []
    for i in text:
        for j in i.split():
            words.append(j.strip())
    return words
corpus = get_corpus(data.review)
corpus[:5]

len(corpus)

from collections import Counter
counter = Counter(corpus)
most_common = counter.most_common(10)
most_common = pd.DataFrame(most_common,columns = ['corpus','count'])
most_common

most_common = most_common.sort_values('count')

plt.figure(figsize =(8,6))
plt.bar(x=most_common['corpus'],height=most_common['count'])
plt.title('Most Common Words in Dataset')

def get_ngrams(review, n, g):
    vec = CountVectorizer(ngram_range=(g, g)).fit(review)
    bag_of_words = vec.transform(review) #sparse matrix of count_vectorizer
    sum_words = bag_of_words.sum(axis=0) #total number of words
    sum_words = np.array(sum_words)[0].tolist() #convert to list
    words_freq = [(word, sum_words[idx]) for word, idx in vec.vocabulary_.items()] #get word freqency for word location in count vec
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True) #key is used to perform sorting using word_freqency 
    return words_freq[:n]

fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,8))
uni_positive = get_ngrams(positive_data,20,1)
uni_positive = dict(uni_positive)
temp = pd.DataFrame(list(uni_positive.items()), columns = ["Common_words" , 'Count'])
sns.barplot(data = temp, x="Count", y="Common_words", orient='h',ax = ax1)
ax1.set_title('Positive reviews')
uni_negative = get_ngrams(negative_data,20,1)
uni_negative = dict(uni_negative)
temp = pd.DataFrame(list(uni_negative.items()), columns = ["Common_words" , 'Count'])
sns.barplot(data = temp, x="Count", y="Common_words", orient='h',ax = ax2)
ax2.set_title('Negative reviews')
fig.suptitle('Unigram analysis for positive and negative reviews')
plt.show()
plt.tight_layout()

fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,8))
bi_positive = get_ngrams(positive_data,20,2)
bi_positive = dict(bi_positive)
temp = pd.DataFrame(list(bi_positive.items()), columns = ["Common_words" , 'Count'])
sns.barplot(data = temp, x="Count", y="Common_words", orient='h',ax = ax1)
ax1.set_title('Positive reviews')
bi_negative = get_ngrams(negative_data,20,2)
bi_negative = dict(bi_negative)
temp = pd.DataFrame(list(bi_negative.items()), columns = ["Common_words" , 'Count'])
sns.barplot(data = temp, x="Count", y="Common_words", orient='h',ax = ax2)
ax2.set_title('Negative reviews')
fig.suptitle('Bigram analysis for positive and negative reviews')
plt.show()
plt.tight_layout()

fig,(ax1,ax2)=plt.subplots(1,2,figsize=(14,18))
tri_positive = get_ngrams(positive_data,20,3)
tri_positive = dict(tri_positive)
temp = pd.DataFrame(list(tri_positive.items()), columns = ["Common_words" , 'Count'])
sns.barplot(data = temp, x="Count", y="Common_words", orient='h',ax = ax1)
ax1.set_title('Positive reviews')
tri_negative = get_ngrams(negative_data,20,3)
tri_negative = dict(tri_negative)
temp = pd.DataFrame(list(tri_negative.items()), columns = ["Common_words" , 'Count'])
sns.barplot(data = temp, x="Count", y="Common_words", orient='h',ax = ax2)
ax2.set_title('Negative reviews')
fig.suptitle('Trigram analysis for positive and negative reviews')
plt.show()

"""## Predictive Modelling using Machine Learning"""

#splitting into train and test
train, test= train_test_split(data, test_size=0.2, random_state=42)
Xtrain, ytrain = train['review'], train['sentiment']
Xtest, ytest = test['review'], test['sentiment']

#Vectorizing data

tfidf_vect = TfidfVectorizer() #tfidfVectorizer
Xtrain_tfidf = tfidf_vect.fit_transform(Xtrain)
Xtest_tfidf = tfidf_vect.transform(Xtest)


#count_vect = CountVectorizer() # CountVectorizer
#Xtrain_count = count_vect.fit_transform(Xtrain)
#Xtest_count = count_vect.transform(Xtest)

"""Logistic Regression"""

lr = LogisticRegression()
lr.fit(Xtrain_tfidf,ytrain)
p1=lr.predict(Xtest_tfidf)
s1=accuracy_score(ytest,p1)
print("Logistic Regression Accuracy :", "{:.2f}%".format(100*s1))
plot_confusion_matrix(lr, Xtest_tfidf, ytest,cmap = 'Blues')
plt.grid(False)

"""Multinomial Naive bayes"""

mnb= MultinomialNB()
mnb.fit(Xtrain_tfidf,ytrain)
p2=mnb.predict(Xtest_tfidf)
s2=accuracy_score(ytest,p2)
print("Multinomial Naive Bayes Classifier Accuracy :", "{:.2f}%".format(100*s2))
plot_confusion_matrix(mnb, Xtest_tfidf, ytest,cmap = 'Blues')
plt.grid(False)

"""SVM"""

linear_svc = LinearSVC(penalty='l2',loss = 'hinge')
linear_svc.fit(Xtrain_tfidf,ytrain)
p3=linear_svc.predict(Xtest_tfidf)
s3=accuracy_score(ytest,p3)
print("Linear Support Vector Classifier Accuracy :", "{:.2f}%".format(100*s3))
plot_confusion_matrix(linear_svc, Xtest_tfidf, ytest,cmap = 'Blues')
plt.grid(False)

"""XGBoost"""

xgbo = XGBClassifier()
xgbo.fit(Xtrain_tfidf,ytrain)
p4=xgbo.predict(Xtest_tfidf)
s4=accuracy_score(ytest,p4)
print("XGBoost Accuracy :", "{:.2f}%".format(100*s4))
plot_confusion_matrix(xgbo, Xtest_tfidf, ytest, cmap = 'Blues')
plt.grid(False)

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier()
rf.fit(Xtrain_tfidf,ytrain)
p5=rf.predict(Xtest_tfidf)
s5=accuracy_score(ytest,p5)
print("Random Forest Accuracy :", "{:.2f}%".format(100*s5))
plot_confusion_matrix(xgbo, Xtest_tfidf, ytest, cmap = 'Blues')
plt.grid(False)

"""## Predictive Modelling Using Deep Learning"""

import nltk
nltk.download('punkt')
lst_story = []
for line in data['review']:
  tokens = word_tokenize(line)
  lst_story.append(tokens)

"""### Training Word2Vec Model"""

import gensim
EMBEDDING_DIM = 100
# train word2vec model
model_wv = gensim.models.Word2Vec(sentences=lst_story,size=EMBEDDING_DIM,window=5,workers=4,min_count=1)

# vocab size
words = list(model_wv.wv.vocab)
print(f'vocabulory size: {len(words)}')

# Save model in ASCII format
filename = 'movie_w2v.txt'
model_wv.wv.save_word2vec_format(filename,binary=False)

model_wv.wv.most_similar('movie')

#Extracting the word embedding from stores file.
import os
embedding_index = {}
f = open(os.path.join('','movie_w2v.txt'),encoding='utf-8')
for line in f:
  values = line.split()
  word = values[0]
  coefs = np.asarray(values[1:])
  embedding_index[word] = coefs
f.close()

# Settting maximum length.
max_len = 500

# vectorize the text samples into 2D integer tensor

tokenizer_obj = Tokenizer(num_words=1000)
tokenizer_obj.fit_on_texts(lst_story)
sequences = tokenizer_obj.texts_to_sequences(lst_story)

# pad sequences
word_index = tokenizer_obj.word_index
review_pad = pad_sequences(sequences,maxlen=max_len)
sentiment = data['sentiment'].values
print('shape of review tensor:',review_pad.shape)
print('shape of sentiment tensor:',sentiment.shape)

len(sequences[50])

review_pad.shape

# splitting data into training set and validatrion set
VALIDATION_SPLIT = 0.2
indices = np.arange(review_pad.shape[0])
np.random.shuffle(indices)
review_pad = review_pad[indices]
sentiment = sentiment[indices]
num_validation_sample = int(VALIDATION_SPLIT*review_pad.shape[0])

X_train_pad = review_pad[:-num_validation_sample]
y_train = sentiment[:-num_validation_sample]
X_test_pad = review_pad[-num_validation_sample:]
y_test = sentiment[-num_validation_sample:]

X_train_pad.shape

print('Shape of review tensor:',review_pad.shape)
print('shape of sentiment tensor:',sentiment.shape)

print('shape of X_train_pad tensor:',X_train_pad.shape)
print('shape of X_test_pad tensor:',X_test_pad.shape)
print('shape of y_train tensor:',y_train.shape)
print('shape of y_test tensor:',y_test.shape)

# Creating Embedding matrix
EMBEDDING_DIM=100
num_words = len(word_index) + 1
embedding_matrix = np.zeros((num_words,EMBEDDING_DIM))

for word, i in word_index.items():
  if i > num_words:
    continue
  embedding_vector = embedding_index.get(word)
  if embedding_vector is not None:
    embedding_matrix[i] = embedding_vector

# It includes the size of vocabulory and the dimention of Word2Vec array.
print(f'Shape of Embedding matrix is : {embedding_matrix.shape}')

"""## Simple RNN Model"""

from keras.initializers import Constant

model_RNN = Sequential()

embedding_layer = Embedding(num_words,EMBEDDING_DIM,
                            weights=[embedding_matrix],
                            input_length=max_len,
                            trainable=False)

model_RNN.add(embedding_layer)
model_RNN.add(SimpleRNN(128,activation='relu',return_sequences=False))
#model_RNN.add(SimpleRNN(256,activation='relu'))
model_RNN.add(Dense(1,activation='sigmoid'))

model_RNN.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
print(model_RNN.summary())

model_RNN.fit(X_train_pad,y_train,batch_size=128,validation_split=0.2,epochs=5,verbose=2)

loss, accuracy = model_RNN.evaluate(X_test_pad,y_test,batch_size=128)

X_test_pad

y_pred = model_RNN.predict(X_test_pad)

y_pred = np.array(y_pred.round())

from sklearn.metrics import confusion_matrix,classification_report,accuracy_score
print(accuracy_score(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))

model_RNN.save("model_RNN.h5")
print("Model saved to disk")

from keras.models import load_model
# load model
model_RNN = load_model('model_RNN.h5')
# summarize model.
model_RNN.summary()

"""## GRU Model"""

# define model
from keras.initializers import Constant

model_GRU = Sequential()

embedding_layer = Embedding(num_words,EMBEDDING_DIM,
                            embeddings_initializer=Constant(embedding_matrix),
                            input_length=max_len,
                            trainable=False)

model_GRU.add(embedding_layer)
model_GRU.add(GRU(128,dropout=0.2, recurrent_dropout=0.2))
model_GRU.add(Dense(1, activation='sigmoid'))

# try using different optimizers and different optimizer configs
model_GRU.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

print('Summary of the built model...')
print(model_GRU.summary())

model_GRU.fit(X_train_pad,y_train,batch_size=128,validation_split=0.2,epochs=5,verbose=2)

loss, accuracy = model_GRU.evaluate(X_test_pad,y_test,batch_size=128)

y_pred = model_GRU.predict(X_test_pad)

y_pred = np.array(y_pred.round())

from sklearn.metrics import confusion_matrix,classification_report,accuracy_score
print(accuracy_score(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))

model_GRU.save("model_GRU.h5")
print("Model saved to disk")

from keras.models import load_model
# load model
model_GRU = load_model('model_GRU.h5')
# summarize model.
#model_GRU.summary()

"""## LSTM Model"""

# define model
model_LSTM = Sequential()

embedding_layer = Embedding(num_words,EMBEDDING_DIM,
                            embeddings_initializer=Constant(embedding_matrix),
                            input_length=max_len,
                            trainable=False)

model_LSTM.add(embedding_layer)
model_LSTM.add(LSTM(128,dropout=0.2,recurrent_dropout=0.2))
#model.add(dropout(0.1))
model_LSTM.add(Dense(1,activation='sigmoid'))

model_LSTM.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
print(model_LSTM.summary())

model_LSTM.fit(X_train_pad,y_train,batch_size=128,validation_split=0.2,epochs=5,verbose=2)

loss, accuracy = model_LSTM.evaluate(X_test_pad,y_test,batch_size=128)

y_pred = model_LSTM.predict(X_test_pad)

y_pred = np.array(y_pred.round())

from sklearn.metrics import confusion_matrix,classification_report,accuracy_score
print(accuracy_score(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))

# save the model to disk
model_LSTM.save("model_LSTM.h5")
print("Model saved to disk")

from keras.models import load_model
# load model
model_LSTM = load_model('model_LSTM.h5')
# summarize model.
model_LSTM.summary()

score = model_LSTM.evaluate(X_test_pad,y_test, verbose=0)
score

"""## CNN Model"""

from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.layers import Flatten
from keras.layers.embeddings import Embedding

model_CNN = Sequential()
e = Embedding(num_words,EMBEDDING_DIM,
                            weights=[embedding_matrix],
                            input_length=max_len,
                            trainable=False)
model_CNN.add(e)
model_CNN.add(Flatten())
model_CNN.add(Dense(256, activation='relu'))
model_CNN.add(Dense(1, activation='sigmoid'))
model_CNN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model_CNN.summary())

model_CNN.fit(X_train_pad,y_train,batch_size=128,validation_split=0.2,epochs=10,verbose=2)

loss, accuracy = model_CNN.evaluate(X_test_pad,y_test,batch_size=128)

y_pred = model_CNN.predict(X_test_pad)

y_pred = np.array(y_pred.round())

from sklearn.metrics import confusion_matrix,classification_report,accuracy_score
print(accuracy_score(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))

# save the model to disk
model_CNN.save("model_CNN.h5")
print("Model saved to disk")

from keras.models import load_model
# load model
model_CNN = load_model('model_CNN.h5')
# summarize model.
model_CNN.summary()